{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f0696c-3bbc-49da-a763-e9f59120eacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Preparing data...\n",
      "üß† Initializing UnetClassifier model...\n",
      "‚öôÔ∏è Using GPU: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "from arcgis.learn import UnetClassifier, DeepLab, PSPNetClassifier, MultiTaskRoadExtractor, prepare_data\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ---- Training settings ----\n",
    "epochsNum = 10 #Maximum times to run the training.\n",
    "batchNum = 4 #Test speeds between batch sizes, typically 2, 4, 8, but can be odd numbers.  (8 causes memory overrun on my pc.)\n",
    "chipSize = 1024 #dataset chip size\n",
    "numWorkers = 4  # Adjust based on your CPU cores\n",
    "\n",
    "# ---- Paths ----\n",
    "training_data_path = r\"C:\\Users\\ss2596\\Documents\\njoko training\\LabeledObjects\\512\"\n",
    "model_output_path = r\"C:\\Users\\ss2596\\Documents\\Njoko_model\\Njoko_model_512\"\n",
    "\n",
    "# ---- Prepare data ----\n",
    "print(\"üì¶ Preparing data...\", flush=True)\n",
    "data = prepare_data(\n",
    "    path=training_data_path,\n",
    "    batch_size=batchNum,\n",
    "    chip_size=chipSize,\n",
    "    num_workers=numWorkers\n",
    ")\n",
    "\n",
    "# ---- Initialize model ----\n",
    "print(\"üß† Initializing UnetClassifier model...\", flush=True)\n",
    "#Uncomment the model you want to use\n",
    "model = UnetClassifier(data)\n",
    "#model = DeepLab(data)\n",
    "#model = PSPNetClassifier(data)\n",
    "#model = MultiTaskRoadExtractor(data)\n",
    "\n",
    "# ---- Create output folder if needed ----\n",
    "os.makedirs(model_output_path, exist_ok=True)\n",
    "\n",
    "# ---- Detect and display GPU device ----\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "    print(f\"‚öôÔ∏è Using GPU: {gpu_name}\", flush=True)\n",
    "else:\n",
    "    print(\"‚öôÔ∏è Using CPU\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7caa539-ffd2-4e2f-b498-e7caf837e940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dice</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.423930</td>\n",
       "      <td>0.368557</td>\n",
       "      <td>0.846940</td>\n",
       "      <td>0.537068</td>\n",
       "      <td>01:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing model metrics...\n",
      "üíæ Checkpoint saved: C:\\Users\\ss2596\\Documents\\Njoko_model\\Njoko_model_512\\checkpoint_epoch_10.dlpk\n",
      "\n",
      "üéØ Final model saved: C:\\Users\\ss2596\\Documents\\Njoko_model\\Njoko_model_512\\final_model.dlpk\n",
      "\n",
      "‚úÖ Training complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "\n",
    "\n",
    "# Define path for saving training metrics\n",
    "metrics_file = os.path.join(model_output_path, \"training_metrics.csv\")\n",
    "\n",
    "# Write header to metrics CSV if it doesn't already exist\n",
    "if not os.path.exists(metrics_file):\n",
    "    with open(metrics_file, mode='w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"epoch\", \"train_loss\", \"valid_loss\", \"accuracy\", \"dice\", \"duration_mins\"])\n",
    "\n",
    "# ---- Early stopping setup ----\n",
    "best_loss = float('inf')     # Lowest validation loss seen so far\n",
    "patience = 3                 # Stop training if no improvement after this many epochs\n",
    "no_improve_epochs = 0        # Counter for consecutive non-improving epochs\n",
    "\n",
    "# ---- Begin training loop ----\n",
    "for epoch in range(epochsNum):\n",
    "    print(f\"\\nüîÅ Starting epoch {epoch + 1}/{epochsNum}...\", flush=True)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train for one epoch\n",
    "    model.fit(1)\n",
    "\n",
    "    # ---- Collect training and validation metrics ----\n",
    "    learner = model.learn\n",
    "    train_loss = learner.recorder.losses[-1].item()               # Last training loss\n",
    "    valid_loss, *metrics = learner.validate()                     # Validation loss and metrics\n",
    "    accuracy = float(metrics[0]) if len(metrics) > 0 else None    # Extract accuracy if available\n",
    "    dice = float(metrics[1]) if len(metrics) > 1 else None        # Extract dice coefficient if available\n",
    "    duration = round((time.time() - start_time) / 60, 2)          # Duration in minutes\n",
    "\n",
    "    # ---- Append metrics to CSV ----\n",
    "    with open(metrics_file, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            epoch + 1,\n",
    "            round(train_loss, 4),\n",
    "            round(valid_loss, 4),\n",
    "            round(accuracy, 4) if accuracy is not None else None,\n",
    "            round(dice, 4) if dice is not None else None,\n",
    "            duration\n",
    "        ])\n",
    "\n",
    "    # ---- Save checkpoint for this epoch ----\n",
    "    checkpoint_path = os.path.join(model_output_path, f\"checkpoint_epoch_{epoch + 1}.dlpk\")\n",
    "    model.save(checkpoint_path, framework='PyTorch')\n",
    "    print(f\"üíæ Checkpoint saved: {checkpoint_path}\", flush=True)\n",
    "\n",
    "\n",
    "    # ---- Early stopping check ----\n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss             # New best model\n",
    "        no_improve_epochs = 0              # Reset counter\n",
    "    else:\n",
    "        no_improve_epochs += 1             # Increment counter\n",
    "        print(f\"üìâ No improvement. {no_improve_epochs} consecutive epochs without improvement.\")\n",
    "\n",
    "    # Trigger early stop if patience limit is reached\n",
    "    if no_improve_epochs >= patience:\n",
    "        print(f\"üõë Early stopping triggered after {patience} epochs without improvement.\")\n",
    "        break\n",
    "\n",
    "# ---- Save final model after training completes or early stopping ----\n",
    "final_model_path = os.path.join(model_output_path, \"final_model.dlpk\")\n",
    "model.save(final_model_path, framework='PyTorch')\n",
    "print(f\"\\nüéØ Final model saved: {final_model_path}\", flush=True)\n",
    "\n",
    "print(\"\\n‚úÖ Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce6018c-a63a-47ca-ab0e-93618db8e61e",
   "metadata": {},
   "source": [
    "| Model                 | Key Features                             | Strengths                       | Typical Use Cases                 | Training Speed     | Model Size / Complexity | Notes                                |\n",
    "|-----------------------|----------------------------------------|--------------------------------|---------------------------------|--------------------|-------------------------|-------------------------------------|\n",
    "| **UNet**              | Encoder-decoder with skip connections  | Good balance of accuracy & speed | General-purpose segmentation    | Fast to Moderate   | Medium                  | Very popular, simple architecture   |\n",
    "| **DeepLab (DeepLabV3)** | Atrous convolutions, ResNet backbone options | High accuracy, good boundary detection | Complex scenes with fine details | Moderate to Slow    | Large                   | High performance, slower to train   |\n",
    "| **PSPNet**            | Pyramid pooling for global context     | Excellent for large-scale context | Scenes with varied background/classes | Moderate          | Large                   | Great for capturing global context  |\n",
    "| **LinkNet**           | Lightweight encoder-decoder architecture | Fast inference, smaller size    | Real-time or resource-constrained use | Fast              | Small                   | Good for speed-critical apps        |\n",
    "| **MultiTaskRoadExtractor** | Specialized for roads extraction       | Accurate for roads, edges       | Road/transportation mapping      | Moderate          | Medium                  | Focused use case, less general      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69400a-4786-4448-9f2e-6ec4536ed20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
